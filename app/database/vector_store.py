# ==============================================================================
# FILE: app/database/vector_store.py - Base de donn√©es vectorielle avec ChromaDB
# ==============================================================================

import asyncio
import logging
import json
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
import uuid

import chromadb
from chromadb.config import Settings
import numpy as np

logger = logging.getLogger(__name__)

class VectorStore:
    """Base de donn√©es vectorielle pour stocker les documents CGI"""
    
    def __init__(self, 
                 persist_directory: str = "./vector_db",
                 collection_name: str = "cgi_documents"):
        """
        Initialize le vector store
        
        Args:
            persist_directory: R√©pertoire de persistance
            collection_name: Nom de la collection ChromaDB
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.is_initialized = False
        
        # M√©tadonn√©es de la collection
        self.collection_metadata = {
            "created_at": None,
            "last_updated": None,
            "document_count": 0,
            "total_tokens": 0
        }
    
    async def initialize(self):
        """Initialize ChromaDB client and collection"""
        if self.is_initialized:
            return
            
        try:
            # Cr√©er le r√©pertoire de persistance
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # Initialiser le client ChromaDB
            self.client = chromadb.PersistentClient(
                path=self.persist_directory,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # Cr√©er ou r√©cup√©rer la collection
            try:
                self.collection = self.client.get_collection(
                    name=self.collection_name
                )
                logger.info(f"üìö Collection existante r√©cup√©r√©e: {self.collection_name}")
                
                # Charger les m√©tadonn√©es
                await self._load_metadata()
                
            except Exception:
                # Cr√©er une nouvelle collection
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={
                        "description": "Documents du Code G√©n√©ral des Imp√¥ts",
                        "created_at": datetime.now().isoformat()
                    }
                )
                logger.info(f"üìö Nouvelle collection cr√©√©e: {self.collection_name}")
                
                # Initialiser les m√©tadonn√©es
                self.collection_metadata = {
                    "created_at": datetime.now().isoformat(),
                    "last_updated": datetime.now().isoformat(),
                    "document_count": 0,
                    "total_tokens": 0
                }
                await self._save_metadata()
            
            self.is_initialized = True
            logger.info("‚úÖ Vector store initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation vector store: {e}")
            raise
    
    async def add_documents(self, documents: List[Dict[str, Any]], 
                          embeddings: List[List[float]]):
        """
        Ajoute des documents avec leurs embeddings
        
        Args:
            documents: Liste des documents √† ajouter
            embeddings: Embeddings correspondants
        """
        if not self.is_initialized:
            await self.initialize()
        
        if len(documents) != len(embeddings):
            raise ValueError("Le nombre de documents doit correspondre au nombre d'embeddings")
        
        try:
            # Pr√©parer les donn√©es pour ChromaDB
            ids = []
            metadatas = []
            texts = []
            
            for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                # ID unique pour chaque document
                doc_id = doc.get('id', f"{uuid.uuid4()}")
                ids.append(doc_id)
                
                # Texte du document
                texts.append(doc['content'])
                
                # M√©tadonn√©es
                metadata = {
                    'title': doc.get('title', ''),
                    'section': doc.get('section', ''),
                    'article': doc.get('article', ''),
                    'source_file': doc.get('source_file', ''),
                    'chunk_index': doc.get('chunk_index', i),
                    'word_count': doc.get('word_count', 0),
                    'char_count': doc.get('char_count', 0),
                    'token_count': doc.get('token_count', 0),
                    'type': doc.get('type', 'content'),
                    'created_at': datetime.now().isoformat()
                }
                
                # Ajouter les m√©tadonn√©es personnalis√©es s'il y en a
                if 'metadata' in doc:
                    metadata.update(doc['metadata'])
                
                metadatas.append(metadata)
            
            # Ajouter √† la collection par batches
            batch_size = 100
            for i in range(0, len(documents), batch_size):
                end_idx = min(i + batch_size, len(documents))
                
                batch_ids = ids[i:end_idx]
                batch_texts = texts[i:end_idx]
                batch_metadatas = metadatas[i:end_idx]
                batch_embeddings = embeddings[i:end_idx]
                
                self.collection.add(
                    ids=batch_ids,
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    embeddings=batch_embeddings
                )
                
                logger.info(f"üì• Batch {i//batch_size + 1} ajout√©: {len(batch_ids)} documents")
            
            # Mettre √† jour les m√©tadonn√©es
            self.collection_metadata['document_count'] += len(documents)
            self.collection_metadata['total_tokens'] += sum(
                doc.get('token_count', 0) for doc in documents
            )
            self.collection_metadata['last_updated'] = datetime.now().isoformat()
            
            await self._save_metadata()
            
            logger.info(f"‚úÖ {len(documents)} documents ajout√©s √† la collection")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ajout documents: {e}")
            raise
    
    async def similarity_search(self, query_embedding: List[float], 
                              top_k: int = 5,
                              filter_criteria: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Recherche par similarit√© vectorielle
        
        Args:
            query_embedding: Embedding de la requ√™te
            top_k: Nombre de r√©sultats √† retourner
            filter_criteria: Crit√®res de filtrage optionnels
            
        Returns:
            Liste des documents les plus similaires
        """
        if not self.is_initialized:
            await self.initialize()
        
        try:
            # Construire le filtre Where si sp√©cifi√©
            where_clause = None
            if filter_criteria:
                where_clause = {}
                for key, value in filter_criteria.items():
                    if value is not None:
                        where_clause[key] = {"$eq": value}
            
            # Effectuer la recherche
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=where_clause,
                include=['documents', 'metadatas', 'distances']
            )
            
            # Traiter les r√©sultats
            formatted_results = []
            
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    doc_id = results['ids'][0][i]
                    document = results['documents'][0][i]
                    metadata = results['metadatas'][0][i]
                    distance = results['distances'][0][i]
                    
                    # Convertir la distance en score de similarit√© (0-1)
                    similarity_score = max(0, 1 - distance)
                    
                    result = {
                        'id': doc_id,
                        'content': document,
                        'similarity_score': round(similarity_score, 4),
                        'distance': round(distance, 4),
                        **metadata  # Ajouter toutes les m√©tadonn√©es
                    }
                    
                    formatted_results.append(result)
            
            logger.info(f"üîç Recherche termin√©e: {len(formatted_results)} r√©sultats")
            return formatted_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche similarit√©: {e}")
            return []
    
    async def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """R√©cup√®re un document par son ID"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            results = self.collection.get(
                ids=[doc_id],
                include=['documents', 'metadatas']
            )
            
            if results['ids'] and len(results['ids']) > 0:
                return {
                    'id': results['ids'][0],
                    'content': results['documents'][0],
                    **results['metadatas'][0]
                }
            
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration document {doc_id}: {e}")
            return None
    
    async def delete_document(self, doc_id: str) -> bool:
        """Supprime un document par son ID"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            self.collection.delete(ids=[doc_id])
            
            # Mettre √† jour les m√©tadonn√©es
            self.collection_metadata['document_count'] = max(0, self.collection_metadata['document_count'] - 1)
            self.collection_metadata['last_updated'] = datetime.now().isoformat()
            await self._save_metadata()
            
            logger.info(f"üóëÔ∏è Document supprim√©: {doc_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur suppression document {doc_id}: {e}")
            return False
    
    async def update_document(self, doc_id: str, new_content: str, 
                            new_embedding: List[float], 
                            new_metadata: Optional[Dict[str, Any]] = None) -> bool:
        """Met √† jour un document existant"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            # ChromaDB ne supporte pas la mise √† jour directe
            # On supprime et on ajoute √† nouveau
            await self.delete_document(doc_id)
            
            # Pr√©parer les nouvelles donn√©es
            documents = [{
                'id': doc_id,
                'content': new_content,
                'metadata': new_metadata or {}
            }]
            
            await self.add_documents(documents, [new_embedding])
            
            logger.info(f"‚úèÔ∏è Document mis √† jour: {doc_id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour document {doc_id}: {e}")
            return False
    
    async def search_by_metadata(self, metadata_filter: Dict[str, Any], 
                               limit: int = 10) -> List[Dict[str, Any]]:
        """Recherche par m√©tadonn√©es uniquement"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            where_clause = {}
            for key, value in metadata_filter.items():
                if value is not None:
                    if isinstance(value, str):
                        where_clause[key] = {"$eq": value}
                    elif isinstance(value, list):
                        where_clause[key] = {"$in": value}
                    else:
                        where_clause[key] = {"$eq": value}
            
            results = self.collection.get(
                where=where_clause,
                limit=limit,
                include=['documents', 'metadatas']
            )
            
            formatted_results = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    result = {
                        'id': results['ids'][i],
                        'content': results['documents'][i],
                        **results['metadatas'][i]
                    }
                    formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche par m√©tadonn√©es: {e}")
            return []
    
    async def get_document_count(self) -> int:
        """Retourne le nombre de documents dans la collection"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            return self.collection.count()
        except Exception as e:
            logger.error(f"‚ùå Erreur comptage documents: {e}")
            return 0
    
    async def has_documents(self) -> bool:
        """V√©rifie s'il y a des documents dans la collection"""
        count = await self.get_document_count()
        return count > 0
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques de la collection"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            count = await self.get_document_count()
            
            # Obtenir quelques √©chantillons pour les stats
            sample_results = self.collection.get(
                limit=10,
                include=['metadatas']
            )
            
            # Calculer des statistiques sur les m√©tadonn√©es
            stats = {
                'total_documents': count,
                'collection_metadata': self.collection_metadata,
                'sample_metadata': sample_results.get('metadatas', [])[:3],
                'collection_name': self.collection_name,
                'persist_directory': self.persist_directory
            }
            
            # Statistiques par type si disponible
            if sample_results.get('metadatas'):
                types = {}
                sources = {}
                
                for metadata in sample_results['metadatas']:
                    doc_type = metadata.get('type', 'unknown')
                    source_file = metadata.get('source_file', 'unknown')
                    
                    types[doc_type] = types.get(doc_type, 0) + 1
                    sources[source_file] = sources.get(source_file, 0) + 1
                
                stats['document_types'] = types
                stats['source_files'] = sources
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration statistiques: {e}")
            return {
                'total_documents': 0,
                'error': str(e)
            }
    
    async def clear(self):
        """Vide compl√®tement la collection"""
        if not self.is_initialized:
            await self.initialize()
        
        try:
            # Supprimer la collection
            self.client.delete_collection(name=self.collection_name)
            
            # Recr√©er une collection vide
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "Documents du Code G√©n√©ral des Imp√¥ts",
                    "created_at": datetime.now().isoformat()
                }
            )
            
            # R√©initialiser les m√©tadonn√©es
            self.collection_metadata = {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "document_count": 0,
                "total_tokens": 0
            }
            
            await self._save_metadata()
            
            logger.info("üßπ Collection vid√©e avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur vidage collection: {e}")
            raise
    
    async def save_indexing_stats(self, stats: Dict[str, Any]):
        """Sauvegarde les statistiques d'indexation"""
        stats_file = os.path.join(self.persist_directory, "indexing_stats.json")
        
        try:
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
                
            logger.info(f"üìä Statistiques d'indexation sauvegard√©es: {stats_file}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur sauvegarde statistiques: {e}")
    
    async def _load_metadata(self):
        """Charge les m√©tadonn√©es de la collection"""
        metadata_file = os.path.join(self.persist_directory, f"{self.collection_name}_metadata.json")
        
        if os.path.exists(metadata_file):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    self.collection_metadata = json.load(f)
                logger.info("üìã M√©tadonn√©es de collection charg√©es")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur chargement m√©tadonn√©es: {e}")
    
    async def _save_metadata(self):
        """Sauvegarde les m√©tadonn√©es de la collection"""
        metadata_file = os.path.join(self.persist_directory, f"{self.collection_name}_metadata.json")
        
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.collection_metadata, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur sauvegarde m√©tadonn√©es: {e}")
    
    async def cleanup(self):
        """Nettoyage des ressources"""
        logger.info("üßπ Nettoyage du vector store...")
        
        # Sauvegarder les m√©tadonn√©es finales
        if self.collection_metadata:
            await self._save_metadata()
        
        # ChromaDB g√®re automatiquement la persistance
        self.client = None
        self.collection = None
        self.is_initialized = False
        
        logger.info("‚úÖ Nettoyage vector store termin√©")