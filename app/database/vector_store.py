# ==============================================================================
# FILE: app/database/vector_store.py - Base de donn√©es vectorielle ultra-l√©g√®re avec scikit-learn
# ==============================================================================

import asyncio
import logging
import json
import os
import pickle
from datetime import datetime
from typing import List, Dict, Any, Optional
import uuid

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class VectorStore:
    """Base de donn√©es vectorielle ultra-l√©g√®re pour stocker les documents CGI"""
    
    def __init__(self, 
                 persist_directory: str = "./vector_db",
                 collection_name: str = "cgi_documents"):
        """
        Initialize le vector store ultra-l√©ger
        
        Args:
            persist_directory: R√©pertoire de persistance
            collection_name: Nom de la collection
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.is_initialized = False
        
        # Stockage en m√©moire (plus rapide)
        self.documents = {}  # id -> document
        self.embeddings = []  # Liste des embeddings
        self.document_ids = []  # Liste des IDs dans l'ordre
        
        # M√©tadonn√©es de la collection
        self.collection_metadata = {
            "created_at": None,
            "last_updated": None,
            "document_count": 0,
            "total_tokens": 0
        }
    
    async def initialize(self):
        """Initialize le vector store ultra-l√©ger"""
        if self.is_initialized:
            return
            
        try:
            # Cr√©er le r√©pertoire de persistance
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # Charger les donn√©es existantes si elles existent
            await self._load_data()
            
            if not self.documents:
                # Initialiser les m√©tadonn√©es pour une nouvelle collection
                self.collection_metadata = {
                    "created_at": datetime.now().isoformat(),
                    "last_updated": datetime.now().isoformat(),
                    "document_count": 0,
                    "total_tokens": 0
                }
                await self._save_data()
            
            self.is_initialized = True
            logger.info("‚úÖ Vector store ultra-l√©ger initialis√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation vector store: {e}")
            raise
    
    async def add_documents(self, 
                           documents: List[Dict[str, Any]], 
                          embeddings: List[List[float]]):
        """
        Ajoute des documents avec leurs embeddings
        
        Args:
            documents: Liste des documents √† ajouter
            embeddings: Liste des embeddings correspondants
        """
        if not self.is_initialized:
            await self.initialize()
        
        try:
            for doc, embedding in zip(documents, embeddings):
                # G√©n√©rer un ID unique
                doc_id = str(uuid.uuid4())
                
                # Stocker le document
                self.documents[doc_id] = {
                    "id": doc_id,
                    "content": doc["content"],
                    "metadata": doc.get("metadata", {}),
                    "created_at": datetime.now().isoformat()
                }
                
                # Stocker l'embedding
                self.embeddings.append(embedding)
                self.document_ids.append(doc_id)
            
            # Mettre √† jour les m√©tadonn√©es
            self.collection_metadata["document_count"] = len(self.documents)
            self.collection_metadata["last_updated"] = datetime.now().isoformat()
            
            # Sauvegarder
            await self._save_data()
            
            logger.info(f"‚úÖ {len(documents)} documents ajout√©s au vector store")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur ajout documents: {e}")
            raise
    
    async def similarity_search(self, query_embedding: List[float], 
                              top_k: int = 5,
                              filter_criteria: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Recherche de similarit√© avec l'embedding de la requ√™te
        
        Args:
            query_embedding: Embedding de la requ√™te
            top_k: Nombre de r√©sultats √† retourner
            filter_criteria: Crit√®res de filtrage (optionnel)
            
        Returns:
            Liste des documents les plus similaires
        """
        if not self.is_initialized or not self.embeddings:
            return []
        
        try:
            # Convertir en arrays numpy
            query_array = np.array(query_embedding).reshape(1, -1)
            embeddings_array = np.array(self.embeddings)
            
            # Calculer les similarit√©s cosinus
            similarities = cosine_similarity(query_array, embeddings_array)[0]
            
            # Cr√©er la liste des r√©sultats avec scores
            results = []
            for i, similarity in enumerate(similarities):
                doc_id = self.document_ids[i]
                document = self.documents[doc_id]
                
                # Appliquer les filtres si sp√©cifi√©s
                if filter_criteria and not self._matches_filter(document, filter_criteria):
                    continue
                
                results.append({
                    "id": doc_id,
                    "content": document["content"],
                    "metadata": document["metadata"],
                    "similarity_score": float(similarity),
                    "created_at": document["created_at"]
                })
            
            # Trier par score de similarit√© d√©croissant
            results.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            # Retourner les top_k r√©sultats
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche similarit√©: {e}")
            return []
    
    def _matches_filter(self, document: Dict[str, Any], filter_criteria: Dict[str, Any]) -> bool:
        """V√©rifie si un document correspond aux crit√®res de filtrage avanc√©"""
        try:
            metadata = document.get("metadata", {})
            
            for key, value in filter_criteria.items():
                # Filtrage par type d'imp√¥t
                if key == "impot_type":
                    impot_types = metadata.get("impot_types", [])
                    if value.upper() not in [it.upper() for it in impot_types]:
                        return False
                
                # Filtrage par r√©gime
                elif key == "regime":
                    regime = metadata.get("regime", "")
                    if regime.upper() != value.upper():
                        return False
                
                # Filtrage par ann√©e de mise √† jour
                elif key == "update_year":
                    update_date = metadata.get("update_date", "")
                    if not update_date or str(value) not in update_date:
                        return False
                
                # Filtrage par cat√©gorie fiscale
                elif key == "fiscal_category":
                    fiscal_category = metadata.get("fiscal_category", "")
                    if fiscal_category.upper() != value.upper():
                        return False
                
                # Filtrage standard (m√©tadonn√©es directes)
                elif key in metadata:
                    if metadata[key] != value:
                        return False
                else:
                    # Si la cl√© n'existe pas dans les m√©tadonn√©es, le filtre √©choue
                    return False
            
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur filtrage: {e}")
            return False
    
    async def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """R√©cup√®re un document par son ID"""
        if not self.is_initialized:
            return None
            
        return self.documents.get(doc_id)
    
    async def delete_document(self, doc_id: str) -> bool:
        """Supprime un document"""
        if not self.is_initialized or doc_id not in self.documents:
            return False
    
        try:
            # Trouver l'index de l'embedding
            if doc_id in self.document_ids:
                idx = self.document_ids.index(doc_id)
                self.embeddings.pop(idx)
                self.document_ids.pop(idx)
            
            # Supprimer le document
            del self.documents[doc_id]
            
            # Mettre √† jour les m√©tadonn√©es
            self.collection_metadata["document_count"] = len(self.documents)
            self.collection_metadata["last_updated"] = datetime.now().isoformat()
            
            await self._save_data()
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur suppression document: {e}")
            return False
    
    async def get_document_count(self) -> int:
        """Retourne le nombre de documents"""
        return len(self.documents) if self.is_initialized else 0
    
    async def has_documents(self) -> bool:
        """V√©rifie s'il y a des documents"""
        return len(self.documents) > 0 if self.is_initialized else False
    
    async def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du vector store"""
        if not self.is_initialized:
            return {}
        
        return {
            "collection_name": self.collection_name,
            "document_count": len(self.documents),
            "embedding_count": len(self.embeddings),
            "embedding_dimension": len(self.embeddings[0]) if self.embeddings else 0,
            "metadata": self.collection_metadata,
            "storage_size_mb": await self._get_storage_size()
        }
    
    async def _get_storage_size(self) -> float:
        """Calcule la taille de stockage en MB"""
        try:
            total_size = 0
            for doc in self.documents.values():
                total_size += len(str(doc))
            for emb in self.embeddings:
                total_size += len(emb) * 8  # 8 bytes par float64
            
            return round(total_size / (1024 * 1024), 2)
        except Exception:
            return 0.0
    
    async def clear(self):
        """Vide compl√®tement le vector store"""
        if not self.is_initialized:
            return
        
        try:
            self.documents.clear()
            self.embeddings.clear()
            self.document_ids.clear()
            
            # R√©initialiser les m√©tadonn√©es
            self.collection_metadata = {
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "document_count": 0,
                "total_tokens": 0
            }
            
            await self._save_data()
            logger.info("üóëÔ∏è Vector store vid√©")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur vidage vector store: {e}")
    
    async def _save_data(self):
        """Sauvegarde les donn√©es sur disque"""
        try:
            data = {
                "documents": self.documents,
                "embeddings": self.embeddings,
                "document_ids": self.document_ids,
                "metadata": self.collection_metadata
            }
            
            filepath = os.path.join(self.persist_directory, f"{self.collection_name}.pkl")
            with open(filepath, 'wb') as f:
                pickle.dump(data, f)
                
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde donn√©es: {e}")
    
    async def _load_data(self):
        """Charge les donn√©es depuis le disque"""
        try:
            filepath = os.path.join(self.persist_directory, f"{self.collection_name}.pkl")
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    data = pickle.load(f)
                
                self.documents = data.get("documents", {})
                self.embeddings = data.get("embeddings", [])
                self.document_ids = data.get("document_ids", [])
                self.collection_metadata = data.get("metadata", self.collection_metadata)
                
                logger.info(f"üìö Donn√©es charg√©es: {len(self.documents)} documents")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible de charger les donn√©es existantes: {e}")
            # Initialiser avec des valeurs par d√©faut
            self.documents = {}
            self.embeddings = []
            self.document_ids = []
    
    async def save_indexing_stats(self, stats: Dict[str, Any]):
        """Sauvegarde les statistiques d'indexation (compatibilit√©)"""
        try:
            # Mettre √† jour les m√©tadonn√©es avec les stats
            if "total_chunks" in stats:
                self.collection_metadata["total_tokens"] = stats.get("total_chunks", 0)
            
            await self._save_data()
            logger.info("üìä Statistiques d'indexation sauvegard√©es")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur sauvegarde statistiques: {e}")
    
    async def cleanup(self):
        """Nettoie les ressources"""
        try:
            await self._save_data()
            logger.info("üßπ Vector store nettoy√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage: {e}")